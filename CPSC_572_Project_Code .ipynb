{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60dc10dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all the necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import matplotlib.colors as colors\n",
    "import ast\n",
    "from scipy.stats import gaussian_kde\n",
    "from scipy.optimize import curve_fit\n",
    "\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07458dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for saving a dataframe as a CSV \n",
    "def save_dataframe_as_csv(dataframe):\n",
    "    dataframe_csv = dataframe.name + \".csv\"\n",
    "    dataframe.to_csv(dataframe_csv, index=False)\n",
    "    return\n",
    "\n",
    "# Processing kaggle dataset and creates a dataframe with the intended date range \n",
    "def create_subset_datasets_from_date_range(date_range_lower, date_range_upper):\n",
    "    df = pd.read_csv('sp500_stocks.csv')\n",
    "    if date_range_lower and date_range_upper:\n",
    "        df = df[(df['Date'] >= date_range_lower) & (df['Date'] < date_range_upper)]\n",
    "        df = df.dropna()\n",
    "        df.reset_index(drop=True, inplace=True)\n",
    "        return df\n",
    "\n",
    "# Pass in a date and duration to get date range according to set duration\n",
    "def create_date_range_from_date(date, duration):\n",
    "    if date and duration:\n",
    "        diff = datetime.timedelta(days = 1)\n",
    "        d = datetime.timedelta(days = duration)\n",
    "        datetime_before_upper = str(date - diff).split(' ')[0]\n",
    "        datetime_before_lower = str(date - diff - d).split(' ')[0]\n",
    "        datetime_after_lower = str(date + diff).split(' ')[0]\n",
    "        datetime_after_upper = str(date + diff + d).split(' ')[0]\n",
    "        \n",
    "        csv_name_before = \"stockMarketDataBefore_\" + str(date).split(' ')[0] + \".csv\"\n",
    "        csv_name_after = \"stockMarketDataAfter_\" + str(date).split(' ')[0] + \".csv\"\n",
    "        df_before = create_subset_datasets_from_date_range(datetime_before_lower, datetime_before_upper)\n",
    "        df_after = create_subset_datasets_from_date_range(datetime_after_lower, datetime_after_upper)\n",
    "        return df_before, df_after\n",
    "\n",
    "# Create a dataframe for the ri(t) of a stock \n",
    "def create_daily_log_closing_price(df):\n",
    "    daily_log_closing_price = pd.DataFrame(columns = df['Date'].unique())\n",
    "    # Drop the first column as it won't apply in the formula\n",
    "    daily_log_closing_price2 = daily_log_closing_price.iloc[:,1:]\n",
    "    daily_log_closing_price2.insert(0, column='Symbol', value = df['Symbol'].unique())\n",
    "    daily_log_closing_price2 = daily_log_closing_price2.fillna(np.float64(0))\n",
    "    return daily_log_closing_price2\n",
    "\n",
    "# Create a dataframe for each stock: log(t) - log(t-1)\n",
    "def create_daily_log_price_all(df, daily_log_closing_price):\n",
    "    average_log_price_all = pd.DataFrame(columns = ['Symbol', 'Average'])\n",
    "    initial_date = df.iloc[0]['Date']\n",
    "    end_date = df.iloc[-1]['Date']\n",
    "    for i, j in df.iterrows():\n",
    "        if j['Date'] == initial_date:\n",
    "            continue\n",
    "        else:\n",
    "            daily_log_closing_price.loc[daily_log_closing_price['Symbol'] == j['Symbol'],j['Date']] = (j['Close'] - df.loc[i-1]['Close'])\n",
    "    return\n",
    "\n",
    "# Create coefficient correlation dataframe\n",
    "def create_create_coefficient_correlation_dataframe(daily_log_closing_price):\n",
    "    daily_log_closing_price_No_Symbols = daily_log_closing_price.drop('Symbol',axis = 1)\n",
    "    daily_log_closing_price_No_Symbols.to_numpy()\n",
    "    CorrelationCoefficient = np.corrcoef(daily_log_closing_price_No_Symbols)\n",
    "    CorrelationCoefficientDF = pd.DataFrame(CorrelationCoefficient, columns = daily_log_closing_price['Symbol'])\n",
    "    CorrelationCoefficientDF.insert(0, column='Symbol', value = daily_log_closing_price['Symbol'].unique())\n",
    "    CorrelationCoefficientDF = add_sector_data_to_dataframe(CorrelationCoefficientDF)\n",
    "    return CorrelationCoefficientDF\n",
    "\n",
    "# Add sector information to the dataframe\n",
    "def add_sector_data_to_dataframe(dataframe):\n",
    "    sectionDF = pd.read_csv(\"constituents_csv.csv\")\n",
    "    dataframe.insert(1,\"Sector\",np.nan)\n",
    "    dataframe['Sector'] = dataframe[\"Symbol\"].map(sectionDF.set_index('Symbol')['Sector'])\n",
    "    return dataframe\n",
    "\n",
    "#Create a graph based on the coefficient matrix given, threshold, or if the information given is a distance value\n",
    "def create_graph(matrix, threshold, distance):\n",
    "    Companies_df = pd.read_csv('sp500_companies.csv')\n",
    "    G = nx.Graph()\n",
    "    \n",
    "    added_stocks = []\n",
    "    for index, row in matrix.iterrows():\n",
    "        if row['Symbol'] not in added_stocks:\n",
    "            added_stocks.append(row['Symbol'])\n",
    "            G.add_node(row['Symbol'], sector = str(row['Sector']), marketcap = Companies_df.loc[Companies_df['Symbol'] == row['Symbol']]['Marketcap'].item())\n",
    "        for colIndex, value in enumerate(row):\n",
    "            if row['Symbol'] == row.index[colIndex] or row.index[colIndex] == 'Symbol' or row.index[colIndex] == 'Sector':\n",
    "                continue\n",
    "            if row.index[colIndex] not in added_stocks:\n",
    "                added_stocks.append(row.index[colIndex])\n",
    "                try:\n",
    "                    tempMarketcap = Companies_df.loc[Companies_df['Symbol'] == row.index[colIndex]]['Marketcap'].item()\n",
    "                except ValueError:\n",
    "                    tempMarketcap = np.nan\n",
    "                G.add_node(row.index[colIndex], sector = str(matrix.loc[matrix['Symbol'] == row.index[colIndex]]['Sector'].item()), marketcap = tempMarketcap)\n",
    "            if threshold:\n",
    "                if value >= threshold:\n",
    "                    G.add_edge(row['Symbol'], row.index[colIndex], correlation = value)\n",
    "            if distance:\n",
    "                if not np.isnan(value):\n",
    "                    G.add_edge(row['Symbol'], row.index[colIndex], distance = value)\n",
    "    return G\n",
    "\n",
    "#Plot graph\n",
    "def plot_graph(G, graph_name):\n",
    "    fig = plt.figure(1,figsize=(40,40))\n",
    "    pos = nx.spring_layout(G, k=0.7, iterations=50)\n",
    "    nx.draw(G, pos, with_labels=True)\n",
    "    graph_name = graph_name + \".jpg\"\n",
    "    plt.savefig(graph_name, format=\"JPG\")\n",
    "\n",
    "#create a graph based on a certain date range\n",
    "def create_graph_for_date_range(date_range_lower, date_range_upper, threshold, graph_name=\"CorrelationCoGraph\"):\n",
    "    df = create_subset_datasets_from_date_range(date_range_lower, date_range_upper)\n",
    "    df['Close'] = np.log(df['Close'])\n",
    "    daily_log_closing_price = create_daily_log_closing_price(df)\n",
    "    create_daily_log_price_all(df, daily_log_closing_price)\n",
    "    G = create_graph_with_daily_log_closing_price(daily_log_closing_price, threshold, graph_name)\n",
    "    return G\n",
    "\n",
    "#create 2 dataframes that give you the random of dates before and after for x duration of days\n",
    "def create_dataframes_specific_before_after(year, month, day, duration=40):\n",
    "    datetime_test = datetime.datetime(year, month, day)\n",
    "    duration = duration\n",
    "    df_before, df_after = create_date_range_from_date(datetime_test, duration)\n",
    "    df_before['Close'] = np.log(df_before['Close'])\n",
    "    df_after['Close'] = np.log(df_after['Close'])\n",
    "    daily_log_closing_price_before = create_daily_log_closing_price(df_before)\n",
    "    create_daily_log_price_all(df_before, daily_log_closing_price_before)\n",
    "    daily_log_closing_price_after = create_daily_log_closing_price(df_after)\n",
    "    create_daily_log_price_all(df_after, daily_log_closing_price_after)\n",
    "    return  daily_log_closing_price_before, daily_log_closing_price_after\n",
    "\n",
    "#create the graph with the daily log closing price\n",
    "def create_graph_with_daily_log_closing_price(daily_log_closing_price, threshold, graph_name=\"Graph\", min_span=False):\n",
    "    CorrelationCo = create_create_coefficient_correlation_dataframe(daily_log_closing_price)\n",
    "    #CorrelationCo.name = \"CorrelationCo\"\n",
    "    #save_dataframe_as_csv(CorrelationCo)\n",
    "    G = create_graph(CorrelationCo, threshold, None)\n",
    "    plot_graph(G, graph_name)\n",
    "    return G\n",
    "\n",
    "#Transform our function into a distance\n",
    "def distance_function(x):\n",
    "    return np.sqrt(2*(1-x))\n",
    "\n",
    "#Create a datafame with distance formula - originally used when we were going to create \n",
    "def create_distance_correlation_coefficient_matrix(CorrelationCo):\n",
    "    distanceDF = CorrelationCo.copy()\n",
    "    distanceDF = distanceDF.loc[:, distanceDF.columns != 'Symbol'].applymap(distance_function)\n",
    "    distanceDF.insert(0,\"Symbol\",CorrelationCo[\"Symbol\"],True)\n",
    "    return distanceDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47996481",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Degreee Distribution \n",
    "## Code used from CPSC 572 course materials: TA materials\n",
    "\n",
    "# probability of finding an edge connecting a j degree node and k degree node\n",
    "# k degree node * j degree node \n",
    "def plot_ejk(list_of_edges_bw_nodes):\n",
    "    \n",
    "    fig = plt.figure()\n",
    "    plt.gca().invert_yaxis()\n",
    "    ax = plt.gca()\n",
    "    ax.set_facecolor('black')\n",
    "    ax.xaxis.tick_top()\n",
    "    x, y = list(), list()\n",
    "    \n",
    "    for edges_bw_nodes in list_of_edges_bw_nodes:\n",
    "        x.append(edges_bw_nodes[0])\n",
    "        y.append(edges_bw_nodes[1])\n",
    "\n",
    "    xy = np.vstack([x,y])\n",
    "    z = gaussian_kde(xy)(xy)\n",
    "\n",
    "    plt.scatter(x, y, norm=colors.LogNorm(vmin=z.min(), vmax=z.max()), c=z, s=1, cmap='afmhot')\n",
    "    plt.colorbar()\n",
    "    \n",
    "    plt.xlabel(\"k\")\n",
    "    plt.ylabel(\"j\")\n",
    "    ax_ = ax.twinx()\n",
    "    plt.ylabel('e_jk')\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "#Plot a Knn graph, modified to allow fo different sample sizes\n",
    "def plot_knn(k, knn_k, SampleNum=50):\n",
    "    \n",
    "    k=np.array(k)\n",
    "    avg_k = np.average(k)\n",
    "    fig = plt.figure()\n",
    "    ax = plt.gca()\n",
    "    \n",
    "    # Binning using a Log Scale\n",
    "    bin_edges = np.logspace(0, np.log10(k.max()), num=SampleNum)\n",
    "    k_nn, _ = np.histogram(knn_k, bins=bin_edges, density=True)\n",
    "    log_be = np.log10(bin_edges)\n",
    "    k = 10**((log_be[1:] + log_be[:-1])/2)\n",
    "    ax.loglog(k, k_nn, marker='o', linestyle='none', label=\"ANND for the original Graph\")\n",
    "    \n",
    "    # Random Network's Average Next Neighbor Degree\n",
    "    neutral_net_knn = np.full(k.shape, np.average(np.square(k)))/avg_k\n",
    "    ax.plot(k, neutral_net_knn, label=\"ANND for the random Graph\")\n",
    "    \n",
    "    # Fitting k_nn = a*(k^u) to the scatter plot to get the value of u\n",
    "    def func(k, a, u):\n",
    "        return a*(k**u)\n",
    "    popt, _ = curve_fit(func, k, k_nn)\n",
    "    a, u = popt\n",
    "    print(\"Value of u is: \", u)\n",
    "    ax.plot(k, func(k, *popt), label=\"Value of u = {0}\".format(u))\n",
    "    \n",
    "    plt.xlabel(\"k\")\n",
    "    plt.ylabel(\"k_nn\")\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "#Plot a degree distribution\n",
    "def plot_degree_distribution(graph):\n",
    "    list_of_edges_bw_nodes = list(nx.node_degree_xy(graph))\n",
    "    plot_ejk(list_of_edges_bw_nodes)\n",
    "    return\n",
    "\n",
    "#Calculate average degree\n",
    "def calculate_average_degree(G):\n",
    "    N = len(G)\n",
    "    L = G.size()\n",
    "    return 2*L/N\n",
    "\n",
    "#Plot degree distribution scale\n",
    "def plot_degree_distribution_scale(G):\n",
    "    degrees = [G.degree(node) for node in G]\n",
    "    kmin = min(degrees)\n",
    "    kmax = max(degrees)\n",
    "    # Get 10 logarithmically spaced bins between kmin and kmax\n",
    "    if kmin>0:\n",
    "        bin_edges = np.logspace(np.log10(kmin), np.log10(kmax), num=20)\n",
    "    else:\n",
    "        bin_edges = np.logspace(0, np.log10(kmax), num=20)\n",
    "    # histogram the data into these bins\n",
    "    density, _ = np.histogram(degrees, bins=bin_edges, density=True)\n",
    "    fig = plt.figure(figsize=(6,4))\n",
    "\n",
    "    # \"x\" should be midpoint (IN LOG SPACE) of each bin\n",
    "    log_be = np.log10(bin_edges)\n",
    "    x = 10**((log_be[1:] + log_be[:-1])/2)\n",
    "    plt.loglog(x, density, marker='o', linestyle='none')\n",
    "    plt.xlabel(r\"degree $k$\", fontsize=16)\n",
    "    plt.ylabel(r\"$P(k)$\", fontsize=16)\n",
    "    \n",
    "    a, b = np.polyfit(x, density, 1)\n",
    "    plt.plot(x, a*x+b)\n",
    "\n",
    "    # remove right and top boundaries because they're ugly\n",
    "    ax = plt.gca()\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.yaxis.set_ticks_position('left')\n",
    "    ax.xaxis.set_ticks_position('bottom')\n",
    "    \n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "    return\n",
    "\n",
    "# Calculate the average next neighbor degree\n",
    "def plot_k_nearest_neighbours(G, sampleNum=50):\n",
    "    node_and_degrees = list(G.degree)\n",
    "\n",
    "    k = list()\n",
    "    knn_k = list()\n",
    "\n",
    "    for node, degree in node_and_degrees:\n",
    "        degrees_of_neighbors = [G.degree(neighbor) for neighbor in list(G.neighbors(node))]\n",
    "        if degree > 0:\n",
    "            avg_neighbor_degree = sum(degrees_of_neighbors)/degree\n",
    "        else:\n",
    "            avg_neighbor_degree = 0\n",
    "\n",
    "    #   print(node, degree, avg_neighbor_degree)\n",
    "        k.append(degree)\n",
    "        knn_k.append(avg_neighbor_degree)\n",
    "\n",
    "    plot_knn(k, knn_k, sampleNum)\n",
    "    return \n",
    "\n",
    "#Create a singular ER network\n",
    "def create_comparative_random_network(G):\n",
    "    NB_NODES = len(G.nodes())\n",
    "    NB_EDGES = len(G.edges())\n",
    "    P = (2 * NB_EDGES) / ((NB_NODES)*(NB_NODES-1))\n",
    "    # Creating a Erdos-Renyi Network\n",
    "    R = nx.erdos_renyi_graph(NB_NODES, P)\n",
    "    return R\n",
    "\n",
    "# Run ensemble of ER networks\n",
    "# From lecture Null Models, modified to only consider connected components \n",
    "def run_ensemble_ER_graphs(G):\n",
    "    clustering_ER = []\n",
    "    short_path_ER = []\n",
    "    degree_ER = []\n",
    "    degree_co_ER = []\n",
    "    \n",
    "    GN = len(G.nodes())\n",
    "    max_L = GN*(GN-1)/2\n",
    "    actual_L = len(G.edges())\n",
    "    p = actual_L/max_L\n",
    "\n",
    "    for i in range(1000): # 1000 is better\n",
    "        ER = nx.erdos_renyi_graph(GN, p, directed=False)\n",
    "        C_ER = np.mean(list(nx.clustering(ER).values()))\n",
    "        tempArr = []\n",
    "        for g in (ER.subgraph(c) for c in nx.connected_components(ER)):\n",
    "            if len(g.edges):\n",
    "                tempArr.append(nx.average_shortest_path_length(g))\n",
    "        degree_co_ER.append(nx.degree_pearson_correlation_coefficient(ER))\n",
    "        degree_ER.append(calculate_average_degree(ER))\n",
    "        clustering_ER.append(C_ER)\n",
    "        short_path_ER.append(np.mean(tempArr))\n",
    "    return clustering_ER, short_path_ER, degree_ER, degree_co_ER\n",
    "\n",
    "# From lecture Null Models\n",
    "def plot_box_plot(plotValues ,value):\n",
    "    fig = plt.figure(figsize=(6,4))\n",
    "\n",
    "    plt.boxplot(plotValues)\n",
    "    plt.plot(2,value,'r',marker='+',markersize=15)\n",
    "    ax = plt.gca()\n",
    "    ax.set_xticks([1,2],labels=[1,2])\n",
    "    plt.xlim([0.5,2.5])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a106e112",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run ensemble of DP graphs\n",
    "# From lecture Null Models\n",
    "def run_ensemble_DP_graphs(G):\n",
    "    DP = G.copy()\n",
    "    clustering_DP = []\n",
    "    short_path_DP = []\n",
    "    degree_DP = []\n",
    "    degree_co_DP = []\n",
    "\n",
    "    for i in range(1000): # 1000 is better\n",
    "\n",
    "        nx.double_edge_swap(DP,nswap=10*G.number_of_edges(),max_tries=100000)\n",
    "        C_DP = np.mean(list(nx.clustering(DP).values()))\n",
    "        tempArr = []\n",
    "        for g in (DP.subgraph(c) for c in nx.connected_components(DP)):\n",
    "            if len(g.edges):\n",
    "                tempArr.append(nx.average_shortest_path_length(g))\n",
    "        degree_co_DP.append(nx.degree_pearson_correlation_coefficient(DP))\n",
    "        degree_DP.append(calculate_average_degree(DP))\n",
    "        clustering_DP.append(C_DP)\n",
    "        short_path_DP.append(np.mean(tempArr))\n",
    "    \n",
    "    return clustering_DP, short_path_DP, degree_DP, degree_co_DP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "973a421a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameters: Date_Range_Lower, Date_Range_Upper, threshold, graph_name (optional)\n",
    "G = create_graph_for_date_range('2020-01-01', '2022-11-01', 0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "294b6ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec85a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.write_gml(G, \"CorrelationCovid75\" + \".gml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "887c242a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameters: year, month, day, duration (optional)\n",
    "#df_before, df_after = create_dataframes_specific_before_after(2021,1,22)\n",
    "#create_graph_with_daily_log_closing_price(df_before, 0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae5f2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create_graph_with_daily_log_closing_price(df_after, 0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e615cd6a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "random_network = create_comparative_random_network(G)\n",
    "print(random_network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f996147a",
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering_ER, short_path_ER, degree_ER, degree_co_ER = run_ensemble_ER_graphs(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e15b06e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "C = nx.average_clustering(G)\n",
    "arr = []\n",
    "for g in (G.subgraph(c) for c in nx.connected_components(G)):\n",
    "    if len(g.edges):\n",
    "        arr.append(nx.average_shortest_path_length(g))\n",
    "d = np.mean(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d6d7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Average Clustering Coefficient: \", C)\n",
    "print(\"Average Clustering Coefficient for 1000 ER: \", np.mean(clustering_ER))\n",
    "print(\"Standard deviation for clustering for 1000 ER: \", np.std(clustering_ER))\n",
    "print(\"-------------------------------------------\")\n",
    "print(\"Average for path length for connected components: \" + str(d))\n",
    "print(\"Average Path length (Connected Componenets) for 1000 ER: \", np.mean(short_path_ER))\n",
    "print(\"Standard deviation for shortest path length (Connected Componenets) for 1000 ER: \", np.std(short_path_ER))\n",
    "print(\"-------------------------------------------\")\n",
    "print(\"Average Degree: \", calculate_average_degree(G))\n",
    "print(\"Average Degree for 1000 ER: \", np.mean(degree_ER))\n",
    "print(\"Standard deviation for Degree for 1000 ER: \", np.std(degree_ER))\n",
    "print(\"Degree Correlation Coefficient for the Original Graph is: \", nx.degree_pearson_correlation_coefficient(G))\n",
    "print(\"Average Degree Correlation Coefficient for 1000 ER: \", np.mean(degree_co_ER))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbfb53d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_box_plot(clustering_ER, C)\n",
    "plot_box_plot(short_path_ER, d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e142ff84",
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering_DP, short_path_DP, degree_DP, degree_co_DP = run_ensemble_DP_graphs(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9cf3c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Average Clustering Coefficient: \", C)\n",
    "print(\"Average Clustering Coefficient for 1000 ER: \", np.mean(clustering_DP))\n",
    "print(\"Standard deviation for clustering for 1000 ER: \", np.std(clustering_DP))\n",
    "print(\"-------------------------------------------\")\n",
    "print(\"Average for path length for connected components: \" + str(d))\n",
    "print(\"Average Path length (Connected Componenets) for 1000 ER: \", np.mean(short_path_DP))\n",
    "print(\"Standard deviation for shortest path length (Connected Componenets) for 1000 ER: \", np.std(short_path_DP))\n",
    "print(\"-------------------------------------------\")\n",
    "print(\"Average Degree: \", calculate_average_degree(G))\n",
    "print(\"Average Degree for 1000 ER: \", np.mean(degree_DP))\n",
    "print(\"Standard deviation for Degree for 1000 ER: \", np.std(degree_DP))\n",
    "print(\"Degree Correlation Coefficient for the Original Graph is: \", nx.degree_pearson_correlation_coefficient(G))\n",
    "print(\"Average Degree Correlation Coefficient for 1000 DP: \", np.mean(degree_co_DP))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26053146",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_box_plot(clustering_DP, C)\n",
    "plot_box_plot(short_path_DP, d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4831172",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_degree_distribution(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b97d8e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_degree_distribution(random_network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21fca652",
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_average_degree(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3e2a52",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_degree_distribution_scale(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c1da56",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_degree_distribution_scale(random_network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789b37cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_average_degree(random_network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e20770e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_k_nearest_neighbours(G,10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
